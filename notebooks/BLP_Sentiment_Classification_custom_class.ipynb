{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5liMiCC1DqV2",
        "outputId": "34f66581-d9ea-43df-fa80-1b98421773a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka_DVEvRDkmM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import math\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ9DjmVJFqyd"
      },
      "outputs": [],
      "source": [
        "def load_text(path,category):\n",
        "  with open(path, 'rb') as f:\n",
        "    texts = []\n",
        "    labels2 = []\n",
        "    for line in f:\n",
        "      texts.append(line.decode(errors='ignore').lower().strip())\n",
        "      labels2.append(category)\n",
        "\n",
        "  return texts, labels2\n",
        "neg_text, neg_labels = load_text('/content/drive/MyDrive/Bengali-Sentiment/all_negative_3307.txt', category=0)\n",
        "pos_text, pos_labels = load_text('/content/drive/MyDrive/Bengali-Sentiment/all_positive_8500.txt', category=1)\n",
        "\n",
        "texts = np.array(neg_text + pos_text)\n",
        "labels = np.array(neg_labels+pos_labels)\n",
        "#labels = np.array([0]*len(neg_text) + [1]*len(pos_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5pbgQiY5m2n"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    text = str(text).replace('।', '\\n')\n",
        "    whitespace = re.compile(u\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\n",
        "    bangla_fullstop = u\"\\u0964\"\n",
        "    punctSeq = u\"['\\\"“”‘’]+|[.?!,…]+|[:;]+\"\n",
        "    punc = u\"[(),$%^&*+={}\\[\\]:\\\"|\\'\\~`<>/,¦!?½£¶¼©⅐⅑⅒⅓⅔⅕⅖⅗⅘⅙⅚⅛⅜⅝⅞⅟↉¤¿º;-]+\"\n",
        "    text = whitespace.sub(\" \", text).strip()\n",
        "    text = re.sub(punctSeq, \" \", text)\n",
        "    text = re.sub(punc, \" \", text)\n",
        "    text = \"\".join(i for i in text if ord(i) > ord('z') or ord(i) == 32)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return (text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUwCYVzvuMnv",
        "outputId": "e893560c-fead-417b-bc52-85f30fd78a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8_SfdSZyb6T"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "def tokenize(texts):\n",
        "  max_len = 0\n",
        "  tokenized_texts = []\n",
        "  word2idx ={} \n",
        "\n",
        "  word2idx['<pad>'] = 0\n",
        "  word2idx['<unk>'] = 1\n",
        "  idx = 2\n",
        "  for sent in texts:\n",
        "    tokenized_sent = sent.split()\n",
        "    tokenized_texts.append(tokenized_sent)\n",
        "    for token in tokenized_sent:\n",
        "      if token not in word2idx:\n",
        "        word2idx[token] = idx\n",
        "        idx += 1\n",
        "    max_len = max(max_len, len(tokenized_sent))\n",
        "  return tokenized_texts, word2idx, max_len\n",
        "\n",
        "def encode(tokenized_texts, word2idx, max_len):\n",
        "  input_ids = []\n",
        "  for tokenized_sent in tokenized_texts:\n",
        "    tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n",
        "    input_id = [word2idx.get(token) for token in tokenized_sent]\n",
        "    input_ids.append(input_id)\n",
        "  return np.array(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSH2amkL4rJg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_glove(word2idx, filenameg, vector_size):\n",
        "  embedding_vectors = np.random.uniform(-0.25, 0.25, (len(word2idx), vector_size))\n",
        "  f = open(filenameg,encoding='utf-8', errors='ignore')\n",
        "  embedding_vectors[word2idx['<pad>']] = np.zeros((vector_size,))\n",
        "  count = 0\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    #print(values[1:])\n",
        "    vectorg = np.asarray(values[1:], dtype=\"float32\")\n",
        "    if word in word2idx:\n",
        "      count += 1\n",
        "      embedding_vectors[word2idx[word]] = vectorg\n",
        "      #print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n",
        "  f.close()\n",
        "  return embedding_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMYlmggazkS6"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm_notebook\n",
        "\n",
        "def load_pretrained_vectors(word2idx, fname):\n",
        "  print(\"Loading pretrained vectors...\")\n",
        "  fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "  n, d = map(int, fin.readline().split())\n",
        "  embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
        "  embeddings[word2idx['<pad>']] = np.zeros((d,))\n",
        "  count = 0\n",
        "  for line in tqdm_notebook(fin):\n",
        "    tokens = line.rstrip().split(' ')\n",
        "    word = tokens[0]\n",
        "    if word in word2idx:\n",
        "      count += 1\n",
        "      embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n",
        "      #print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n",
        "  return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhruT7lX0HZa",
        "outputId": "cad7aeff-9013-4404-9863-c45a22484cb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing...\n",
            "\n",
            "24250\n"
          ]
        }
      ],
      "source": [
        "# Tokenize, build vocabulary, encode tokens\n",
        "print(\"Tokenizing...\\n\")\n",
        "tokenized_texts, word2idx, max_len = tokenize(texts)\n",
        "input_ids = encode(tokenized_texts, word2idx, max_len)\n",
        "\n",
        "print(len(word2idx))\n",
        "embedding_dim = 200\n",
        "# Load pretrained vectors\n",
        "embeddings = load_glove(word2idx, '/content/drive/MyDrive/Bengali-Sentiembedding/GloVe-300.txt',200)\n",
        "embeddings = torch.tensor(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOpJ6BDHO8HV"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,SequentialSampler)\n",
        "\n",
        "def data_loader(train_inputs, val_inputs, train_labels, val_labels,batch_size=20):\n",
        "  train_inputs, val_inputs, train_labels, val_labels =\\\n",
        "  tuple(torch.tensor(data) for data in[train_inputs, val_inputs, train_labels, val_labels])\n",
        "  batch_size = 50\n",
        "  train_data = TensorDataset(train_inputs, train_labels)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "  val_data = TensorDataset(val_inputs, val_labels)\n",
        "  val_sampler = SequentialSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "  return train_dataloader, val_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isOWX66z2zh-"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "train_dataloader, val_dataloader =data_loader(train_inputs, val_inputs, train_labels, val_labels, batch_size=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5RsWcchF9vJ"
      },
      "outputs": [],
      "source": [
        "class CNN_NLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN_NLP, self).__init__()\n",
        "    pretrained_embedding=embeddings\n",
        "    freeze_embedding=False\n",
        "    vocab_size=len(word2idx)\n",
        "    embed_dim=200\n",
        "    filter_sizes=[3, 4, 5]\n",
        "    num_filters=[100, 100, 100]\n",
        "    num_classes=2\n",
        "    dropout=0.5\n",
        "    self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
        "    self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,freeze=freeze_embedding)\n",
        "    self.conv1 =  nn.Conv1d(in_channels=self.embed_dim,out_channels=100,kernel_size=3)\n",
        "    self.conv2 =  nn.Conv1d(in_channels=self.embed_dim,out_channels=100,kernel_size=4)\n",
        "    self.conv3 =  nn.Conv1d(in_channels=self.embed_dim,out_channels=100,kernel_size=5)\n",
        "    self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "  def forward(self, input_ids):\n",
        "    x_embed = self.embedding(input_ids).float()\n",
        "    x_reshaped = x_embed.permute(0, 2, 1)\n",
        "    x1 = F.relu(self.conv1(x_reshaped))\n",
        "    x2 = F.relu(self.conv2(x_reshaped))\n",
        "    x3 = F.relu(self.conv3(x_reshaped))\n",
        "    x1 = F.max_pool1d(x1,kernel_size=x1.shape[2])\n",
        "    x2 = F.max_pool1d(x2,kernel_size=x2.shape[2])\n",
        "    x3 = F.max_pool1d(x3,kernel_size=x3.shape[2])\n",
        "    fc_x = torch.cat([x1.squeeze(dim=2),x2.squeeze(dim=2),x3.squeeze(dim=2)], dim=1)\n",
        "    logits = self.fc(self.dropout(fc_x))\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naMGmzXOIuL1"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def initilize_model():\n",
        "  cnn_model = CNN_NLP()\n",
        "  cnn_model.to(device)\n",
        "  optimizer = optim.Adadelta(cnn_model.parameters(),lr=0.25,rho=0.95)\n",
        "  return cnn_model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlY9mOS5Jb1U"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\"\"\"\n",
        "\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, optimizer, train_dataloader, val_dataloader=None, epochs=10):\n",
        "  best_accuracy = 0\n",
        "  print(\"Start training...\\n\")\n",
        "  print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "  print(\"-\"*60)\n",
        "  best_model = copy.deepcopy( model.state_dict() )\n",
        "  for epoch_i in range(epochs):\n",
        "    t0_epoch = time.time()\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "      model.zero_grad()\n",
        "      logits = model(b_input_ids)\n",
        "      #print('logits',logits)\n",
        "      #print('b_labels',b_labels)\n",
        "      loss = loss_fn(logits, b_labels)\n",
        "      total_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      avg_train_loss = total_loss / len(train_dataloader)\n",
        "    #print('Loss: ',total_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "    for batch in val_dataloader:\n",
        "      b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "      with torch.no_grad():\n",
        "        logits = model(b_input_ids)\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "      #print(preds)\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "    #print('val_accuracy: ',val_accuracy)\n",
        "    if val_accuracy > best_accuracy:\n",
        "      best_accuracy = val_accuracy\n",
        "      best_model = copy.deepcopy( model.state_dict() )\n",
        "      time_elapsed = time.time() - t0_epoch\n",
        "      print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "  return best_model\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "  model.eval()\n",
        "  val_accuracy = []\n",
        "  val_loss = []\n",
        "  y_act = []\n",
        "  y_predict = []\n",
        "  for batch in val_dataloader:\n",
        "    b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "    with torch.no_grad():\n",
        "      logits = model(b_input_ids)\n",
        "      loss = loss_fn(logits, b_labels)\n",
        "      val_loss.append(loss.item())\n",
        "      preds = torch.argmax(logits, dim=1).flatten()\n",
        "      y_predict.append(preds)\n",
        "      y_act.append(b_labels)\n",
        "      #print(preds)\n",
        "      accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "      val_accuracy.append(accuracy)\n",
        "  val_loss = np.mean(val_loss)\n",
        "  val_accuracy = np.mean(val_accuracy)\n",
        "  #print('actual', y_act)\n",
        "  #print('actual', y_predict)\n",
        "  return val_loss, val_accuracy, y_act, y_predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-I-kjliLSdw",
        "outputId": "794792c0-3d81-40a2-f507-7621d2eafaba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "------------------------------------------------------------\n",
            "   1    |   0.376780   |  0.282692  |   87.45   |   9.35   \n",
            "   2    |   0.272258   |  0.246872  |   89.11   |   2.64   \n",
            "   3    |   0.237653   |  0.232639  |   89.87   |   2.64   \n",
            "   6    |   0.181832   |  0.214648  |   91.25   |   2.64   \n",
            "   8    |   0.152395   |  0.204678  |   91.70   |   2.65   \n",
            "  10    |   0.119540   |  0.204209  |   91.79   |   2.67   \n",
            "  13    |   0.087221   |  0.209128  |   92.09   |   2.67   \n",
            "  17    |   0.057026   |  0.213858  |   92.35   |   2.69   \n"
          ]
        }
      ],
      "source": [
        "set_seed(42)\n",
        "cnn_non_static, optimizer = initilize_model()\n",
        "best_model = train(cnn_non_static, optimizer, train_dataloader, val_dataloader, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m_T1RKIVBYD"
      },
      "outputs": [],
      "source": [
        "torch.save(best_model,'/content/drive/MyDrive/Bengali-Sentiment_model/Bengali-Sentiment-CNN.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-tlgUoVWQqc",
        "outputId": "8d34e356-7147-4b69-fb76-719091d62641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.sampler.BatchSampler object at 0x7f300043a3a0>\n",
            "Final_model_Accuracy 92.34785456927612\n"
          ]
        }
      ],
      "source": [
        "cnn_model = CNN_NLP()\n",
        "cnn_model.to(device)\n",
        "print(val_dataloader.batch_sampler)\n",
        "cnn_model.load_state_dict(torch.load('/content/drive/MyDrive/Bengali-Sentiment_model/Bengali-Sentiment-CNN.pt'))\n",
        "val_loss, val_accuracy, y_act, y_predict = evaluate(cnn_model,val_dataloader)\n",
        "print(\"Final_model_Accuracy\", val_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "y = [t.cpu().numpy() for t in y_act]\n",
        "yp = [t.cpu().numpy() for t in y_predict]\n",
        "y = [item for sublist in y for item in sublist]\n",
        "yp = [item for sublist in yp for item in sublist]\n",
        "print(metrics.classification_report(y, yp))\n",
        "print(metrics.confusion_matrix(y, yp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjU7VtptfD6u",
        "outputId": "a0750d74-0852-4ebc-82e5-027c91a84e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       999\n",
            "           1       0.95      0.94      0.95      2544\n",
            "\n",
            "    accuracy                           0.92      3543\n",
            "   macro avg       0.90      0.91      0.91      3543\n",
            "weighted avg       0.92      0.92      0.92      3543\n",
            "\n",
            "[[ 883  116]\n",
            " [ 155 2389]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}