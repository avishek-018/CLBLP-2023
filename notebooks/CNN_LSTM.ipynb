{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3MYekhRSbGh",
        "outputId": "d77b15a8-d063-4f60-9310-23dd796f2567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import math\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "L3maVoLHSuOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text(path,category):\n",
        "  with open(path, 'rb') as f:\n",
        "    texts = []\n",
        "    labels2 = []\n",
        "    for line in f:\n",
        "      texts.append(line.decode(errors='ignore').lower().strip())\n",
        "      labels2.append(category)\n",
        "\n",
        "  return texts, labels2\n",
        "neg_text, neg_labels = load_text('/content/drive/MyDrive/Bengali-Sentiment/all_negative_3307.txt', category=0)\n",
        "pos_text, pos_labels = load_text('/content/drive/MyDrive/Bengali-Sentiment/all_positive_8500.txt', category=1)\n",
        "\n",
        "texts = np.array(neg_text + pos_text)\n",
        "labels = np.array(neg_labels+pos_labels)\n",
        "#labels = np.array([0]*len(neg_text) + [1]*len(pos_text))"
      ],
      "metadata": {
        "id": "EMEGAqsvTErZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = str(text).replace('।', '\\n')\n",
        "    whitespace = re.compile(u\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\n",
        "    bangla_fullstop = u\"\\u0964\"\n",
        "    punctSeq = u\"['\\\"“”‘’]+|[.?!,…]+|[:;]+\"\n",
        "    punc = u\"[(),$%^&*+={}\\[\\]:\\\"|\\'\\~`<>/,¦!?½£¶¼©⅐⅑⅒⅓⅔⅕⅖⅗⅘⅙⅚⅛⅜⅝⅞⅟↉¤¿º;-]+\"\n",
        "    text = whitespace.sub(\" \", text).strip()\n",
        "    text = re.sub(punctSeq, \" \", text)\n",
        "    text = re.sub(punc, \" \", text)\n",
        "    text = \"\".join(i for i in text if ord(i) > ord('z') or ord(i) == 32)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return (text)\n"
      ],
      "metadata": {
        "id": "VEHv0_2OTLpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN4HLemsTOin",
        "outputId": "b2f5745e-ca74-48fc-97ca-407e788fcb23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "def tokenize(texts):\n",
        "  max_len = 0\n",
        "  tokenized_texts = []\n",
        "  word2idx ={} \n",
        "\n",
        "  word2idx['<pad>'] = 0\n",
        "  word2idx['<unk>'] = 1\n",
        "  idx = 2\n",
        "  for sent in texts:\n",
        "    tokenized_sent = sent.split()\n",
        "    tokenized_texts.append(tokenized_sent)\n",
        "    for token in tokenized_sent:\n",
        "      if token not in word2idx:\n",
        "        word2idx[token] = idx\n",
        "        idx += 1\n",
        "    max_len = max(max_len, len(tokenized_sent))\n",
        "  return tokenized_texts, word2idx, max_len\n",
        "\n",
        "def encode(tokenized_texts, word2idx, max_len):\n",
        "  input_ids = []\n",
        "  for tokenized_sent in tokenized_texts:\n",
        "    tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n",
        "    input_id = [word2idx.get(token) for token in tokenized_sent]\n",
        "    input_ids.append(input_id)\n",
        "  return np.array(input_ids)"
      ],
      "metadata": {
        "id": "6XERqKpmTT1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_glove(word2idx, filenameg, vector_size):\n",
        "  embedding_vectors = np.random.uniform(-0.25, 0.25, (len(word2idx), vector_size))\n",
        "  f = open(filenameg,encoding='utf-8', errors='ignore')\n",
        "  embedding_vectors[word2idx['<pad>']] = np.zeros((vector_size,))\n",
        "  count = 0\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    #print(values[1:])\n",
        "    vectorg = np.asarray(values[1:], dtype=\"float32\")\n",
        "    if word in word2idx:\n",
        "      count += 1\n",
        "      embedding_vectors[word2idx[word]] = vectorg\n",
        "      #print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n",
        "  f.close()\n",
        "  return embedding_vectors"
      ],
      "metadata": {
        "id": "wBaUu5-cTYFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,SequentialSampler)\n",
        "\n",
        "def data_loader(train_inputs, val_inputs, train_labels, val_labels,batch_size=50):\n",
        "  train_inputs, val_inputs, train_labels, val_labels =\\\n",
        "  tuple(torch.tensor(data) for data in[train_inputs, val_inputs, train_labels, val_labels])\n",
        "  batch_size = 50\n",
        "  train_data = TensorDataset(train_inputs, train_labels)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "  val_data = TensorDataset(val_inputs, val_labels)\n",
        "  val_sampler = SequentialSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "  return train_dataloader, val_dataloader"
      ],
      "metadata": {
        "id": "c7KtId4dTyUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize, build vocabulary, encode tokens\n",
        "print(\"Tokenizing...\\n\")\n",
        "tokenized_texts, word2idx, max_len = tokenize(texts)\n",
        "input_ids = encode(tokenized_texts, word2idx, max_len)\n",
        "\n",
        "print(len(word2idx))\n",
        "embedding_dim = 200\n",
        "# Load pretrained vectors\n",
        "embeddings = load_glove(word2idx, '/content/drive/MyDrive/Bengali-Sentiembedding/GloVe-300.txt',200)\n",
        "embeddings = torch.tensor(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qutCqOGXTb4f",
        "outputId": "f43e2cea-e968-4dc2-8b33-0f3090e66671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing...\n",
            "\n",
            "24250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, test_size=0.1, random_state=42)\n",
        "\n",
        "train_dataloader, val_dataloader =\\\n",
        "data_loader(train_inputs, val_inputs, train_labels, val_labels, batch_size=50)"
      ],
      "metadata": {
        "id": "iiLBTWdITnO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_NLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN_NLP, self).__init__()\n",
        "    pretrained_embedding=embeddings\n",
        "    freeze_embedding=False\n",
        "    vocab_size=len(word2idx)\n",
        "    embed_dim=200\n",
        "    filter_sizes=[3, 4, 5]\n",
        "    num_filters=[100, 100, 100]\n",
        "    num_classes=2\n",
        "    dropout=0.5\n",
        "    self.num_layers = 2\n",
        "    self.hidden_dim=256\n",
        "    self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
        "    self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,freeze=freeze_embedding)\n",
        "    self.conv1 =  nn.Conv1d(in_channels=self.embed_dim,out_channels=100,kernel_size=3)\n",
        "    self.conv2 =  nn.Conv1d(in_channels=self.embed_dim,out_channels=100,kernel_size=4)\n",
        "    self.conv3 =  nn.Conv1d(in_channels=self.embed_dim,out_channels=100,kernel_size=5)\n",
        "    self.lstm = nn.LSTM(300,256,self.num_layers,bidirectional=True,batch_first=True)\n",
        "    self.fc = nn.Linear(512, num_classes)\n",
        "    #self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "  def forward(self, input_ids):\n",
        "    x_embed = self.embedding(input_ids).float()\n",
        "    x_reshaped = x_embed.permute(0, 2, 1)\n",
        "    x1 = F.relu(self.conv1(x_reshaped))\n",
        "    x2 = F.relu(self.conv2(x_reshaped))\n",
        "    x3 = F.relu(self.conv3(x_reshaped))\n",
        "    x1 = F.max_pool1d(x1,kernel_size=x1.shape[2])\n",
        "    x2 = F.max_pool1d(x2,kernel_size=x2.shape[2])\n",
        "    x3 = F.max_pool1d(x3,kernel_size=x3.shape[2])\n",
        "    fc_x = torch.cat([x1.squeeze(dim=2),x2.squeeze(dim=2),x3.squeeze(dim=2)], dim=1)\n",
        "    batch_size = input_ids.shape[0]  \n",
        "    h = torch.zeros((self.num_layers*2, self.hidden_dim))\n",
        "    c = torch.zeros((self.num_layers*2, self.hidden_dim))\n",
        "    torch.nn.init.xavier_normal_(h)\n",
        "    torch.nn.init.xavier_normal_(c)\n",
        "    h = h.to(device)\n",
        "    c = c.to(device)\n",
        "    out, (hidden, cell) = self.lstm(fc_x, (h,c))\n",
        "    out =  F.relu(out)\n",
        "    out = self.dropout(out)\n",
        "    #print(out)\n",
        "    out = self.fc(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "wavCcgFoT6TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def initilize_model():\n",
        "  cnn_model = CNN_NLP()\n",
        "  cnn_model.to(device)\n",
        "  optimizer = optim.Adadelta(cnn_model.parameters(),lr=0.25,rho=0.95)\n",
        "  return cnn_model, optimizer"
      ],
      "metadata": {
        "id": "-FLBlDYiWCPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\"\"\"\n",
        "\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, optimizer, train_dataloader, val_dataloader=None, epochs=10):\n",
        "  best_accuracy = 0\n",
        "  print(\"Start training...\\n\")\n",
        "  print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "  print(\"-\"*60)\n",
        "  best_model = copy.deepcopy( model.state_dict() )\n",
        "  for epoch_i in range(epochs):\n",
        "    t0_epoch = time.time()\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "      model.zero_grad()\n",
        "      logits = model(b_input_ids)\n",
        "      loss = loss_fn(logits, b_labels)\n",
        "      total_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    if val_dataloader is not None:\n",
        "      val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "      #print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\") \n",
        "      if val_accuracy > best_accuracy:\n",
        "        best_accuracy = val_accuracy\n",
        "        best_model = copy.deepcopy( model.state_dict() )\n",
        "        time_elapsed = time.time() - t0_epoch\n",
        "        print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")        \n",
        "  print(\"\\n\")\n",
        "  print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
        "  return best_model\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "  model.eval()\n",
        "  val_accuracy = []\n",
        "  val_loss = []\n",
        "  for batch in val_dataloader:\n",
        "    b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "    with torch.no_grad():\n",
        "      logits = model(b_input_ids)\n",
        "      loss = loss_fn(logits, b_labels)\n",
        "      val_loss.append(loss.item())\n",
        "      preds = torch.argmax(logits, dim=1).flatten()\n",
        "      accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "      val_accuracy.append(accuracy)\n",
        "  val_loss = np.mean(val_loss)\n",
        "  val_accuracy = np.mean(val_accuracy)\n",
        "  return val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "ba9aH0mzWH6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "cnn_non_static, optimizer = initilize_model()\n",
        "best_model = train(cnn_non_static, optimizer, train_dataloader, val_dataloader, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkMDz22EWPdb",
        "outputId": "a1a9e39d-b94e-4427-eaaa-155ffef4ff00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "------------------------------------------------------------\n",
            "   1    |   0.583357   |  0.531831  |   71.59   |   4.57   \n",
            "   2    |   0.431690   |  0.325480  |   85.30   |   4.71   \n",
            "   3    |   0.301634   |  0.255597  |   89.26   |   4.66   \n",
            "   4    |   0.246326   |  0.235336  |   89.76   |   4.54   \n",
            "   5    |   0.214474   |  0.218859  |   91.78   |   5.50   \n",
            "   9    |   0.111365   |  0.206079  |   92.12   |   4.58   \n",
            "  10    |   0.085604   |  0.212796  |   92.28   |   4.60   \n",
            "  15    |   0.024495   |  0.265907  |   92.62   |   4.51   \n",
            "  17    |   0.017046   |  0.266146  |   92.65   |   4.73   \n",
            "  18    |   0.016812   |  0.282680  |   92.81   |   4.60   \n",
            "  19    |   0.014582   |  0.287448  |   93.03   |   5.04   \n",
            "  29    |   0.003086   |  0.327794  |   93.20   |   4.52   \n",
            "  41    |   0.003484   |  0.343565  |   93.28   |   4.51   \n",
            "\n",
            "\n",
            "Training complete! Best accuracy: 93.28%.\n"
          ]
        }
      ]
    }
  ]
}